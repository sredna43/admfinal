---
title: "Mushroom Classification"
author: "Paul Heltemes and Ander Olson"
date: "5/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FNN)
library(tidyverse)
library(rpart)
```


We will be examining how to best predict whether a mushroom is poisonous based on other properties. 

First, we will load in the data and look at a simple visualization of it.

```{r}
mush.df <- read.csv("mushrooms.csv")
```

Since there are many predictors at play, let's look at color versus whether it is poisonous.

```{r}
plot(mush.df$cap.color,mush.df$class,xlab="color",ylab="Edible(e) or Poisonous(p)")
```

In this case, n=brown,b=buff, c=cinnamon,g=gray,r=green,p=pink,u=purple,e=red,w=white and y=yellow. We can see that if a person used color alone, for most of the more common mushrooms, you would be stuck at a near 50% success, which is the same as guessing randomly on a 2 factor classification. We do see that for some of the really rare colors, purple and green, they are 100% edible which might be make color a necessary predictor to distinguish the class of these few points.

Let's first run KNN.

```{r}
calcErrorKnnClass <- function(kVal,data.df)
{
  numFolds <- 10
  n <- nrow(data.df)
  folds <- sample(1:numFolds,n,rep=T)
  errs <- numeric(numFolds)
  for(fold in 1:numFolds){
   train.df <- data.df[folds != fold,] 
   test.df <- data.df[folds == fold,]  
   train.x <- data.matrix(train.df[,2:23])
   resp.x<- data.matrix(train.df[c("class")])
   test.x <- data.matrix(test.df[,2:23])
   mod.knn <- knn(train.x,test.x,resp.x,k=kVal)
   classes <- c("p","e") 
   errs[fold] <- with(test.df,mean(class != classes[(mod.knn==1)+1]))
  }
  (mean(errs))
}
```

```{r}
err <- vector()
err[1] <- calcErrorKnnClass(1,mush.df)
for(i in c(3:50))
{
  err[i-1] <- calcErrorKnnClass(i,mush.df)
}
Knn_error <- min(err)
(Knn_error)
```

That's a very small error, on some tests it is even 0. 

Let's graph this to see what is happening to get such a small minimum.

```{r}
res.df <- data.frame(kVal=c(1,seq(3,50)),mse=err)
plot(res.df)
```


Now let's take a look at a Linear Classification Model.

First, let's look into the data a little bit to see if there are any factors that we can disregard.

```{r}
str(mush.df)
```

Looks like veil.type is useless as it only has 1 factor.

```{r}
mush.df$veil.type <- NULL
```

Let's just see a little bit more about our data.

```{r}
table(mush.df$class)
```
## GLM

Now let's try using glm
```{r}
calcErrorGLMClass <- function(test_formula){
   frm <- as.formula(test_formula)
   data.df <- mush.df
   ## Get rid of odor for now
   data.df <- data.df %>%
      mutate(class = ifelse(class == 'e',0,1))
   data.df$odor <- NULL
   numFolds <- 10
   n <- nrow(data.df)
   folds <- sample(1:numFolds,n,rep=T)
   errs <- numeric(numFolds)
   for(fold in 1:numFolds){
      train.df <- data.df[folds != fold,] 
      test.df <- data.df[folds == fold,]
      mod.lm <- glm(frm, 
                    data=train.df,
                    family=binomial())
      test.df$preds <- predict(mod.lm, 
                               newdata = test.df, 
                               type = "response")
      test.df <- test.df %>%
         mutate(preds = ifelse(preds > 0.5, 1, 0))
      errs[fold] <- with(test.df,mean(class != preds))
   }
   (mean(errs))
}
calcErrorGLMClass("class ~ .")
```

Well, the model doesn't converge when we use all of the factors. Essentially, it seems like you truly can fit a model perfectly with mushrooms if you're given their attributes.

## LM

How about just LM?

```{r}
calcErrorLMClass <- function(test_formula){
   frm <- as.formula(test_formula)
   data.df <- mush.df
   ## Get rid of odor for now
   data.df <- data.df %>%
      mutate(class = ifelse(class == 'e',0,1))
   data.df$odor <- NULL
   numFolds <- 10
   n <- nrow(data.df)
   folds <- sample(1:numFolds,n,rep=T)
   errs <- numeric(numFolds)
   for(fold in 1:numFolds){
      train.df <- data.df[folds != fold,] 
      test.df <- data.df[folds == fold,]
      mod.lm <- lm(frm, 
                    data=train.df,
                    family=binomial())
      test.df$preds <- predict(mod.lm, 
                               newdata = test.df, 
                               type = "response")
      test.df <- test.df %>%
         mutate(preds = ifelse(preds > 0.5, 1, 0))
      errs[fold] <- with(test.df,mean(class != preds))
   }
   (mean(errs))
}
calcErrorLMClass("class ~ .")
```

We get a few errors (unsurprisingly) but we also get a pretty low error rate... extremely low. However, someone could still die!

Since these models are giving us such great results, let's see if we can find some meaningful subset of predictors that we could potentially use in the wild. Before we do that, it seems like odor is by far the best predictor of edible/poisonous so let's take a look at that.

## PLOTS

```{r}
library(ggplot2)
ggplot(mush.df, aes(x = class, y = odor, col = class)) +
   geom_jitter(alpha = 0.5) +
   scale_color_manual(breaks = c("e", "p"),
                      values = c("green", "red"),
                      labels = c("Edible", "Poisonous"))+
   labs(title = "Mushroom classification based on odor")
```

If it smells bad, don't eat it.

What if we lose our sense of smell? Well, let's assume that we are still able to see clearly.

```{r}
ggplot(mush.df, aes(x = cap.shape, y = cap.color, col = class)) +
   geom_jitter(alpha = 0.5) +
   scale_color_manual(breaks = c("e", "p"),
                      values = c("green", "red"),
                      labels = c("Edible", "Poisonous"))+
   labs(title = "Mushroom classification based on cap color and shape")
```

Do gills have anything to do with it?

```{r}
ggplot(mush.df, aes(x = gill.size, y = gill.spacing, col = class)) +
   geom_jitter(alpha = 0.5) +
   scale_color_manual(breaks = c("e", "p"),
                      values = c("green", "red"),
                      labels = c("Edible", "Poisonous"))+
   labs(title = "Mushroom classification based on gill size and spacing")
```

## RPART

```{r}
calcErrorRpartClass <- function(test_formula){
   frm <- as.formula(test_formula)
   data.df <- mush.df
   ## Get rid of odor for now
   data.df$odor <- NULL
   numFolds <- 10
   n <- nrow(data.df)
   folds <- sample(1:numFolds,n,rep=T)
   errs <- numeric(numFolds)
   for(fold in 1:numFolds){
      train.df <- data.df[folds != fold,] 
      test.df <- data.df[folds == fold,]
      mod.rpart <- rpart(frm, 
                    data=train.df,
                    method = "class",
                    cp = 0.00001)
      test.df$preds <- predict(mod.rpart, 
                               newdata = test.df, 
                               type = "class")
      errs[fold] <- with(test.df,mean(class != preds))
   }
   (mean(errs))
}
calcErrorRpartClass("class ~ cap.color + cap.surface")
```
